# G4F (GPT4Free) é›†æˆæŒ‡å— / G4F Integration Guide

[English Version](#english-version) | [ä¸­æ–‡ç‰ˆæœ¬](#ä¸­æ–‡ç‰ˆæœ¬)

---

## ä¸­æ–‡ç‰ˆæœ¬

### ç›®å½•
- [ä»€ä¹ˆæ˜¯ G4F](#ä»€ä¹ˆæ˜¯-g4f)
- [ä¸ºä»€ä¹ˆä½¿ç”¨ G4F](#ä¸ºä»€ä¹ˆä½¿ç”¨-g4f)
- [é›†æˆæ–¹æ¡ˆ](#é›†æˆæ–¹æ¡ˆ)
- [å®ç°æ­¥éª¤](#å®ç°æ­¥éª¤)
- [å®Œæ•´ç¤ºä¾‹ä»£ç ](#å®Œæ•´ç¤ºä¾‹ä»£ç )
- [é…ç½®å’Œä½¿ç”¨](#é…ç½®å’Œä½¿ç”¨)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

### ä»€ä¹ˆæ˜¯ G4F

G4F (GPT4Free) æ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæä¾›äº†å…è´¹è®¿é—®å¤šä¸ª AI æ¨¡å‹çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬ GPT-4ã€GPT-3.5ã€Claudeã€Llama ç­‰ã€‚å®ƒé€šè¿‡é€†å‘å·¥ç¨‹å„ç§ AI æœåŠ¡æä¾›å•†çš„ APIï¼Œè®©å¼€å‘è€…æ— éœ€ API å¯†é’¥å³å¯ä½¿ç”¨è¿™äº›æ¨¡å‹ã€‚

**é¡¹ç›®åœ°å€**: https://github.com/xtekky/gpt4free

**ä¸»è¦ç‰¹ç‚¹**ï¼š
- ğŸ†“ å®Œå…¨å…è´¹ï¼Œæ— éœ€ API å¯†é’¥
- ğŸŒ æ”¯æŒå¤šä¸ª AI æä¾›å•†ï¼ˆOpenAI, Anthropic, Google ç­‰ï¼‰
- ğŸ”„ è‡ªåŠ¨åˆ‡æ¢æä¾›å•†ï¼ˆå½“ä¸€ä¸ªä¸å¯ç”¨æ—¶ï¼‰
- ğŸ’¬ æ”¯æŒæµå¼å“åº”
- ğŸ¯ ç®€å•æ˜“ç”¨çš„ Python API

### ä¸ºä»€ä¹ˆä½¿ç”¨ G4F

åœ¨ AI Chat é¡¹ç›®ä¸­é›†æˆ G4F çš„ä¼˜åŠ¿ï¼š

1. **é™ä½æˆæœ¬**: æ— éœ€æ”¯ä»˜æ˜‚è´µçš„ API è´¹ç”¨
2. **å¿«é€ŸåŸå‹**: é€‚åˆå¼€å‘å’Œæµ‹è¯•é˜¶æ®µ
3. **å¤šæ¨¡å‹æ”¯æŒ**: ä¸€ä¸ªé€‚é…å™¨å³å¯è®¿é—®å¤šä¸ª AI æ¨¡å‹
4. **çµæ´»æ€§**: å¯ä»¥è½»æ¾åˆ‡æ¢ä¸åŒçš„æä¾›å•†
5. **ç¤¾åŒºæ´»è·ƒ**: G4F é¡¹ç›®æŒç»­æ›´æ–°å’Œç»´æŠ¤

**âš ï¸ æ³¨æ„äº‹é¡¹**ï¼š
- G4F é€‚åˆå¼€å‘å’Œæµ‹è¯•ç¯å¢ƒï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨å®˜æ–¹ API
- æŸäº›æä¾›å•†å¯èƒ½ä¸ç¨³å®šæˆ–æœ‰ä½¿ç”¨é™åˆ¶
- éœ€è¦éµå®ˆå„ AI æœåŠ¡æä¾›å•†çš„ä½¿ç”¨æ¡æ¬¾

### é›†æˆæ–¹æ¡ˆ

#### æ–¹æ¡ˆä¸€ï¼šç›´æ¥é›†æˆï¼ˆPython å­è¿›ç¨‹ï¼‰

é€šè¿‡ Node.js è°ƒç”¨ Python è„šæœ¬æ¥ä½¿ç”¨ G4Fã€‚

**ä¼˜ç‚¹**ï¼š
- ç›´æ¥ä½¿ç”¨ G4F çš„ Python åº“
- åŠŸèƒ½å®Œæ•´ï¼Œæ”¯æŒæ‰€æœ‰ç‰¹æ€§
- æ›´æ–°æ–¹ä¾¿

**ç¼ºç‚¹**ï¼š
- éœ€è¦å®‰è£… Python ç¯å¢ƒ
- è·¨è¯­è¨€é€šä¿¡æœ‰æ€§èƒ½å¼€é”€
- éƒ¨ç½²ç¨å¾®å¤æ‚

#### æ–¹æ¡ˆäºŒï¼šHTTP ä»£ç†æœåŠ¡

ä½¿ç”¨ G4F çš„ API æœåŠ¡å™¨æ¨¡å¼ã€‚

**ä¼˜ç‚¹**ï¼š
- æœåŠ¡è§£è€¦ï¼Œæ˜“äºæ‰©å±•
- å¯ä»¥ç‹¬ç«‹éƒ¨ç½²å’Œæ‰©å®¹
- æ”¯æŒå¤šä¸ªå®¢æˆ·ç«¯å…±äº«

**ç¼ºç‚¹**ï¼š
- éœ€è¦é¢å¤–çš„æœåŠ¡ç®¡ç†
- ç½‘ç»œå»¶è¿Ÿ

#### æ–¹æ¡ˆä¸‰ï¼šä½¿ç”¨ç¤¾åŒº Node.js åŒ…è£…

ä½¿ç”¨ç¤¾åŒºæä¾›çš„ Node.js åŒ…è£…åº“ï¼ˆå¦‚ `g4f`ï¼‰ã€‚

**ä¼˜ç‚¹**ï¼š
- çº¯ JavaScript/TypeScript å®ç°
- éƒ¨ç½²ç®€å•
- æ€§èƒ½å¥½

**ç¼ºç‚¹**ï¼š
- åŠŸèƒ½å¯èƒ½ä¸å¦‚å®˜æ–¹ Python åº“å®Œæ•´
- ä¾èµ–ç¤¾åŒºç»´æŠ¤

**æœ¬æŒ‡å—æ¨èæ–¹æ¡ˆäºŒï¼ˆHTTP ä»£ç†æœåŠ¡ï¼‰**ï¼Œå› ä¸ºå®ƒæä¾›äº†æœ€å¥½çš„çµæ´»æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚

### å®ç°æ­¥éª¤

#### æ­¥éª¤ 1: å®‰è£…å’Œå¯åŠ¨ G4F API æœåŠ¡å™¨

é¦–å…ˆï¼Œå®‰è£… G4Fï¼š

```bash
pip install -U g4f
```

å¯åŠ¨ G4F API æœåŠ¡å™¨ï¼š

```bash
python -m g4f.api --port 1337 --bind 0.0.0.0
```

æˆ–è€…åˆ›å»ºä¸€ä¸ªå¯åŠ¨è„šæœ¬ `scripts/start-g4f-server.py`ï¼š

```python
from g4f.api import run_api

if __name__ == '__main__':
    run_api(
        host='0.0.0.0',
        port=1337,
        debug=False
    )
```

è¿è¡Œï¼š

```bash
python scripts/start-g4f-server.py
```

#### æ­¥éª¤ 2: åˆ›å»º G4F é€‚é…å™¨

åœ¨ `src/ai/adapters/` ç›®å½•ä¸‹åˆ›å»º `G4FAdapter.ts`ï¼š

```typescript
import { randomUUID } from "crypto";
import { Message } from "../../types/Message";
import { AIResponse, StreamingChunk, StreamingParams } from "../../types/Responses";
import { AIAdapter } from "./AIAdapter";

interface G4FMessage {
  role: string;
  content: string;
}

interface G4FRequest {
  model: string;
  messages: G4FMessage[];
  stream?: boolean;
  temperature?: number;
  max_tokens?: number;
}

interface G4FStreamChunk {
  id: string;
  choices: Array<{
    delta: {
      content?: string;
    };
    finish_reason?: string;
  }>;
}

export class G4FAdapter implements AIAdapter {
  public readonly modelId: string;
  public readonly displayName: string;
  public readonly supportsStreaming = true;

  private readonly baseURL: string;
  private readonly timeout: number;

  constructor(
    modelId: string = 'gpt-3.5-turbo',
    displayName?: string,
    options: {
      baseURL?: string;
      timeout?: number;
    } = {}
  ) {
    this.modelId = modelId;
    this.displayName = displayName ?? this.getDefaultDisplayName(modelId);
    this.baseURL = options.baseURL ?? 'http://localhost:1337';
    this.timeout = options.timeout ?? 60000;
  }

  async sendMessage(
    messages: Message[],
    options: Partial<StreamingParams> = {}
  ): Promise<AIResponse> {
    const g4fMessages = this.convertMessages(messages);
    const requestBody: G4FRequest = {
      model: this.modelId,
      messages: g4fMessages,
      stream: false,
      temperature: options.temperature,
      max_tokens: options.maxTokens,
    };

    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), this.timeout);

    try {
      const response = await fetch(`${this.baseURL}/v1/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
        signal: controller.signal,
      });

      clearTimeout(timeoutId);

      if (!response.ok) {
        throw new Error(`G4F API error: ${response.status} ${response.statusText}`);
      }

      const data = await response.json();
      const content = data.choices?.[0]?.message?.content ?? '';

      return {
        id: data.id ?? randomUUID(),
        model: this.modelId,
        content,
        createdAt: new Date(),
        usage: {
          inputTokens: this.estimateTokens(g4fMessages.map(m => m.content).join('\n')),
          outputTokens: this.estimateTokens(content),
        },
      };
    } catch (error) {
      clearTimeout(timeoutId);
      if ((error as Error).name === 'AbortError') {
        throw new Error('G4F request timeout');
      }
      throw error;
    }
  }

  async streamMessage(
    messages: Message[],
    options: Partial<StreamingParams>,
    onChunk: (chunk: StreamingChunk) => void
  ): Promise<AIResponse> {
    const g4fMessages = this.convertMessages(messages);
    const requestBody: G4FRequest = {
      model: this.modelId,
      messages: g4fMessages,
      stream: true,
      temperature: options.temperature,
      max_tokens: options.maxTokens,
    };

    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), this.timeout);

    try {
      const response = await fetch(`${this.baseURL}/v1/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
        signal: controller.signal,
      });

      clearTimeout(timeoutId);

      if (!response.ok) {
        throw new Error(`G4F API error: ${response.status} ${response.statusText}`);
      }

      let fullContent = '';
      let chunkIndex = 0;

      const reader = response.body?.getReader();
      const decoder = new TextDecoder();

      if (!reader) {
        throw new Error('Response body is not readable');
      }

      while (true) {
        const { done, value } = await reader.read();
        
        if (done) break;

        const chunk = decoder.decode(value, { stream: true });
        const lines = chunk.split('\n').filter(line => line.trim().startsWith('data: '));

        for (const line of lines) {
          const data = line.replace('data: ', '').trim();
          
          if (data === '[DONE]') {
            break;
          }

          try {
            const parsed: G4FStreamChunk = JSON.parse(data);
            const content = parsed.choices?.[0]?.delta?.content;

            if (content) {
              fullContent += content;
              onChunk({
                id: parsed.id ?? randomUUID(),
                model: this.modelId,
                index: chunkIndex++,
                content,
                done: false,
                createdAt: new Date(),
              });
            }

            if (parsed.choices?.[0]?.finish_reason) {
              onChunk({
                id: parsed.id ?? randomUUID(),
                model: this.modelId,
                index: chunkIndex++,
                content: '',
                done: true,
                createdAt: new Date(),
              });
            }
          } catch (e) {
            // å¿½ç•¥è§£æé”™è¯¯ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€è¡Œ
            console.warn('Failed to parse G4F stream chunk:', e);
          }
        }
      }

      return {
        id: randomUUID(),
        model: this.modelId,
        content: fullContent,
        createdAt: new Date(),
        usage: {
          inputTokens: this.estimateTokens(g4fMessages.map(m => m.content).join('\n')),
          outputTokens: this.estimateTokens(fullContent),
        },
      };
    } catch (error) {
      clearTimeout(timeoutId);
      if ((error as Error).name === 'AbortError') {
        throw new Error('G4F stream timeout');
      }
      throw error;
    }
  }

  private convertMessages(messages: Message[]): G4FMessage[] {
    return messages.map(msg => ({
      role: msg.role === 'assistant' ? 'assistant' : msg.role === 'system' ? 'system' : 'user',
      content: msg.content,
    }));
  }

  private estimateTokens(content: string): number {
    // ç²—ç•¥ä¼°ç®—ï¼šè‹±æ–‡çº¦ 4 å­—ç¬¦/tokenï¼Œä¸­æ–‡çº¦ 2 å­—ç¬¦/token
    const chineseChars = (content.match(/[\u4e00-\u9fa5]/g) || []).length;
    const otherChars = content.length - chineseChars;
    return Math.ceil(chineseChars / 2 + otherChars / 4);
  }

  private getDefaultDisplayName(modelId: string): string {
    const displayNames: Record<string, string> = {
      'gpt-4': 'GPT-4',
      'gpt-4-turbo': 'GPT-4 Turbo',
      'gpt-3.5-turbo': 'GPT-3.5 Turbo',
      'claude-3-opus': 'Claude 3 Opus',
      'claude-3-sonnet': 'Claude 3 Sonnet',
      'llama-2-70b': 'Llama 2 70B',
      'gemini-pro': 'Gemini Pro',
    };
    return displayNames[modelId] ?? modelId.toUpperCase();
  }
}
```

#### æ­¥éª¤ 3: åœ¨ä¼šè¯æœåŠ¡ä¸­æ³¨å†Œ G4F é€‚é…å™¨

ä¿®æ”¹ `src/chat/SequentialSessionService.ts`ï¼Œæ›´æ–° `getAdapter` æ–¹æ³•ï¼š

```typescript
import { G4FAdapter } from "../ai/adapters/G4FAdapter";

export class SequentialSessionService {
  // ... å…¶ä»–ä»£ç  ...

  private getAdapter(modelId: string): AIAdapter {
    if (!this.adapters.has(modelId)) {
      // æ ¹æ® modelId åˆ¤æ–­ä½¿ç”¨å“ªä¸ªé€‚é…å™¨
      if (modelId.startsWith('mock-')) {
        this.adapters.set(modelId, new MockAdapter(modelId));
      } else {
        // ä½¿ç”¨ G4F é€‚é…å™¨
        this.adapters.set(modelId, new G4FAdapter(modelId));
      }
    }
    return this.adapters.get(modelId)!;
  }

  // ... å…¶ä»–ä»£ç  ...
}
```

#### æ­¥éª¤ 4: é…ç½®ç¯å¢ƒå˜é‡

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```bash
# æœåŠ¡ç«¯å£
PORT=3000

# G4F API æœåŠ¡å™¨åœ°å€
G4F_API_URL=http://localhost:1337

# G4F è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
G4F_TIMEOUT=60000
```

å®‰è£… dotenvï¼š

```bash
npm install dotenv
```

åœ¨ `src/index.ts` ä¸­åŠ è½½ç¯å¢ƒå˜é‡ï¼š

```typescript
import dotenv from 'dotenv';
dotenv.config();
```

æ›´æ–° `G4FAdapter` æ„é€ å‡½æ•°ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼š

```typescript
constructor(
  modelId: string = 'gpt-3.5-turbo',
  displayName?: string,
  options: {
    baseURL?: string;
    timeout?: number;
  } = {}
) {
  this.modelId = modelId;
  this.displayName = displayName ?? this.getDefaultDisplayName(modelId);
  this.baseURL = options.baseURL ?? process.env.G4F_API_URL ?? 'http://localhost:1337';
  this.timeout = options.timeout ?? Number(process.env.G4F_TIMEOUT ?? 60000);
}
```

### å®Œæ•´ç¤ºä¾‹ä»£ç 

#### åˆ›å»ºä½¿ç”¨ G4F çš„èŠå¤©å®¤

```typescript
import { store } from './storage/InMemoryStore';

const g4fRoom = store.createChatRoom({
  name: 'G4F æŠ€æœ¯è®¨è®ºå®¤',
  userId: 'demo-user',
  defaultMode: 'sequential',
  aiMembers: [
    {
      id: randomUUID(),
      modelId: 'gpt-4',
      displayName: 'GPT-4 æ¶æ„å¸ˆ',
      order: 1,
      isEnabled: true,
      config: {
        systemPrompt: 'ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„è½¯ä»¶æ¶æ„å¸ˆï¼Œæ“…é•¿ç³»ç»Ÿè®¾è®¡ã€‚',
        temperature: 0.7,
        maxTokens: 1000,
        responseStyle: 'professional',
      },
    },
    {
      id: randomUUID(),
      modelId: 'claude-3-sonnet',
      displayName: 'Claude ä»£ç å®¡æŸ¥å‘˜',
      order: 2,
      isEnabled: true,
      config: {
        systemPrompt: 'ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ä»£ç å®¡æŸ¥ä¸“å®¶ï¼Œæ³¨é‡ä»£ç è´¨é‡ã€‚',
        temperature: 0.5,
        maxTokens: 800,
        responseStyle: 'detailed',
      },
    },
    {
      id: randomUUID(),
      modelId: 'llama-2-70b',
      displayName: 'Llama æ€§èƒ½ä¼˜åŒ–å¸ˆ',
      order: 3,
      isEnabled: true,
      config: {
        systemPrompt: 'ä½ ä¸“æ³¨äºæ€§èƒ½ä¼˜åŒ–å’Œèµ„æºç®¡ç†ã€‚',
        temperature: 0.6,
        maxTokens: 800,
        responseStyle: 'technical',
      },
    },
  ],
});
```

#### æµ‹è¯• G4F é›†æˆ

åˆ›å»º `scripts/test-g4f.ts`ï¼š

```typescript
import { G4FAdapter } from '../src/ai/adapters/G4FAdapter';
import { Message } from '../src/types/Message';

async function testG4F() {
  console.log('ğŸ§ª Testing G4F integration...\n');

  const adapter = new G4FAdapter('gpt-3.5-turbo', 'GPT-3.5 Test');

  const messages: Message[] = [
    {
      id: '1',
      role: 'user',
      content: 'Hello! Can you explain what TypeScript is in one sentence?',
      createdAt: new Date(),
    },
  ];

  try {
    console.log('ğŸ“¤ Sending message to G4F...');
    const response = await adapter.sendMessage(messages);
    console.log('âœ… Response received:');
    console.log(`   Model: ${response.model}`);
    console.log(`   Content: ${response.content}`);
    console.log(`   Tokens: ${response.usage.outputTokens}\n`);

    console.log('ğŸ“¤ Testing streaming...');
    let streamedContent = '';
    await adapter.streamMessage(
      messages,
      {},
      (chunk) => {
        streamedContent += chunk.content;
        process.stdout.write(chunk.content);
      }
    );
    console.log('\nâœ… Streaming completed\n');
    console.log(`   Full content length: ${streamedContent.length} chars`);

  } catch (error) {
    console.error('âŒ Error:', (error as Error).message);
    console.error('   Make sure G4F API server is running on http://localhost:1337');
  }
}

testG4F();
```

ç¼–è¯‘å¹¶è¿è¡Œï¼š

```bash
npm run build
node dist/scripts/test-g4f.js
```

æˆ–ä½¿ç”¨ tsxï¼š

```bash
npx tsx scripts/test-g4f.ts
```

### é…ç½®å’Œä½¿ç”¨

#### Docker Compose é…ç½®

åˆ›å»º `docker-compose.yml`ï¼š

```yaml
version: '3.8'

services:
  # G4F API æœåŠ¡å™¨
  g4f-api:
    image: python:3.11-slim
    working_dir: /app
    command: >
      sh -c "pip install -U g4f &&
             python -m g4f.api --port 1337 --bind 0.0.0.0"
    ports:
      - "1337:1337"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:1337/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # AI Chat æœåŠ¡
  ai-chat:
    build: .
    ports:
      - "3000:3000"
    environment:
      - PORT=3000
      - G4F_API_URL=http://g4f-api:1337
      - G4F_TIMEOUT=60000
    depends_on:
      - g4f-api
    restart: unless-stopped
```

å¯åŠ¨æœåŠ¡ï¼š

```bash
docker-compose up -d
```

#### æ”¯æŒçš„ G4F æ¨¡å‹

G4F æ”¯æŒä¼—å¤šæ¨¡å‹ï¼Œå¸¸ç”¨çš„åŒ…æ‹¬ï¼š

**OpenAI ç³»åˆ—**:
- `gpt-4`
- `gpt-4-turbo`
- `gpt-3.5-turbo`

**Anthropic ç³»åˆ—**:
- `claude-3-opus`
- `claude-3-sonnet`
- `claude-3-haiku`

**å¼€æºæ¨¡å‹**:
- `llama-2-70b`
- `llama-2-13b`
- `mistral-7b`
- `mixtral-8x7b`

**Google ç³»åˆ—**:
- `gemini-pro`
- `palm-2`

æŸ¥çœ‹æœ€æ–°æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ï¼š
```bash
python -c "from g4f import models; print([m.name for m in models.ModelUtils.convert])"
```

### æœ€ä½³å®è·µ

#### 1. é”™è¯¯å¤„ç†å’Œé‡è¯•

```typescript
class G4FAdapter implements AIAdapter {
  private async fetchWithRetry(
    url: string,
    options: RequestInit,
    maxRetries: number = 3
  ): Promise<Response> {
    let lastError: Error | null = null;

    for (let i = 0; i < maxRetries; i++) {
      try {
        const response = await fetch(url, options);
        if (response.ok) return response;
        
        // å¦‚æœæ˜¯ 5xx é”™è¯¯ï¼Œé‡è¯•
        if (response.status >= 500) {
          lastError = new Error(`Server error: ${response.status}`);
          await this.delay(1000 * (i + 1)); // æŒ‡æ•°é€€é¿
          continue;
        }
        
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      } catch (error) {
        lastError = error as Error;
        if (i < maxRetries - 1) {
          await this.delay(1000 * (i + 1));
        }
      }
    }

    throw lastError ?? new Error('Max retries exceeded');
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}
```

#### 2. è¯·æ±‚é™æµ

```typescript
import pLimit from 'p-limit';

class G4FAdapter implements AIAdapter {
  private static requestLimit = pLimit(5); // æœ€å¤š 5 ä¸ªå¹¶å‘è¯·æ±‚

  async sendMessage(messages: Message[], options?: Partial<StreamingParams>): Promise<AIResponse> {
    return G4FAdapter.requestLimit(() => this._sendMessage(messages, options));
  }

  private async _sendMessage(
    messages: Message[],
    options?: Partial<StreamingParams>
  ): Promise<AIResponse> {
    // åŸæ¥çš„å®ç°
  }
}
```

#### 3. ç¼“å­˜å“åº”

```typescript
import NodeCache from 'node-cache';

class G4FAdapter implements AIAdapter {
  private cache = new NodeCache({ stdTTL: 3600 }); // ç¼“å­˜ 1 å°æ—¶

  async sendMessage(messages: Message[], options?: Partial<StreamingParams>): Promise<AIResponse> {
    const cacheKey = this.getCacheKey(messages, options);
    const cached = this.cache.get<AIResponse>(cacheKey);
    
    if (cached) {
      return { ...cached, id: randomUUID() }; // è¿”å›ç¼“å­˜ç»“æœ
    }

    const response = await this._sendMessage(messages, options);
    this.cache.set(cacheKey, response);
    return response;
  }

  private getCacheKey(messages: Message[], options?: Partial<StreamingParams>): string {
    return JSON.stringify({ modelId: this.modelId, messages, options });
  }
}
```

#### 4. ç›‘æ§å’Œæ—¥å¿—

```typescript
class G4FAdapter implements AIAdapter {
  private logger = createLogger('G4FAdapter');

  async sendMessage(messages: Message[], options?: Partial<StreamingParams>): Promise<AIResponse> {
    const startTime = Date.now();
    
    this.logger.info('Sending message to G4F', {
      modelId: this.modelId,
      messageCount: messages.length,
    });

    try {
      const response = await this._sendMessage(messages, options);
      const duration = Date.now() - startTime;
      
      this.logger.info('G4F response received', {
        modelId: this.modelId,
        duration,
        outputTokens: response.usage.outputTokens,
      });

      return response;
    } catch (error) {
      this.logger.error('G4F request failed', {
        modelId: this.modelId,
        duration: Date.now() - startTime,
        error: (error as Error).message,
      });
      throw error;
    }
  }
}
```

### å¸¸è§é—®é¢˜

#### Q1: G4F API æœåŠ¡å™¨æ— æ³•å¯åŠ¨

**A**: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
1. ç¡®ä¿å·²å®‰è£… G4F: `pip install -U g4f`
2. ç«¯å£ 1337 æ˜¯å¦è¢«å ç”¨: `lsof -i :1337`
3. æŸ¥çœ‹ G4F æ—¥å¿—è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯

#### Q2: è¯·æ±‚è¶…æ—¶æˆ–å¤±è´¥

**A**: 
1. å¢åŠ è¶…æ—¶æ—¶é—´ï¼šè®¾ç½® `G4F_TIMEOUT` ç¯å¢ƒå˜é‡
2. G4F å¯èƒ½ä¼šè‡ªåŠ¨åˆ‡æ¢æä¾›å•†ï¼ŒæŸäº›æä¾›å•†å¯èƒ½æš‚æ—¶ä¸å¯ç”¨
3. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™è®¾ç½®

#### Q3: æŸäº›æ¨¡å‹ä¸å¯ç”¨

**A**: 
1. G4F çš„æä¾›å•†çŠ¶æ€ä¼šåŠ¨æ€å˜åŒ–
2. å°è¯•åˆ‡æ¢åˆ°å…¶ä»–æ¨¡å‹
3. æŸ¥çœ‹ G4F é¡¹ç›®çš„æœ€æ–°çŠ¶æ€: https://github.com/xtekky/gpt4free

#### Q4: å“åº”è´¨é‡ä¸ç¨³å®š

**A**: 
1. ä¸åŒæä¾›å•†çš„è´¨é‡å¯èƒ½æœ‰å·®å¼‚
2. è€ƒè™‘æ·»åŠ å“åº”éªŒè¯å’Œé‡è¯•é€»è¾‘
3. ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨å®˜æ–¹ API

#### Q5: å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨

**A**: 
G4F æ›´é€‚åˆå¼€å‘å’Œæµ‹è¯•ã€‚å¯¹äºç”Ÿäº§ç¯å¢ƒï¼š
1. ä½¿ç”¨å®˜æ–¹ API æœåŠ¡
2. å®ç°å®˜æ–¹ API é€‚é…å™¨ï¼ˆOpenAI, Anthropic ç­‰ï¼‰
3. ä¿ç•™ G4F ä½œä¸ºé™çº§å¤‡ç”¨æ–¹æ¡ˆ

#### Q6: å¦‚ä½•å¤„ç†å¹¶å‘è¯·æ±‚

**A**: 
1. ä½¿ç”¨è¯·æ±‚é˜Ÿåˆ—é™åˆ¶å¹¶å‘æ•°
2. å®ç°è¯·æ±‚é™æµå’Œé€€é¿ç­–ç•¥
3. è€ƒè™‘éƒ¨ç½²å¤šä¸ª G4F å®ä¾‹åšè´Ÿè½½å‡è¡¡

---

## English Version

### Table of Contents
- [What is G4F](#what-is-g4f)
- [Why Use G4F](#why-use-g4f)
- [Integration Approaches](#integration-approaches)
- [Implementation Steps](#implementation-steps)
- [Complete Example Code](#complete-example-code)
- [Configuration and Usage](#configuration-and-usage)
- [Best Practices](#best-practices-1)
- [FAQ](#faq)

### What is G4F

G4F (GPT4Free) is an open-source project that provides free access to multiple AI models including GPT-4, GPT-3.5, Claude, Llama, and more. It reverse-engineers various AI service providers' APIs, allowing developers to use these models without API keys.

**Project URL**: https://github.com/xtekky/gpt4free

**Key Features**:
- ğŸ†“ Completely free, no API keys required
- ğŸŒ Supports multiple AI providers (OpenAI, Anthropic, Google, etc.)
- ğŸ”„ Automatic provider switching (when one is unavailable)
- ğŸ’¬ Supports streaming responses
- ğŸ¯ Simple and easy-to-use Python API

### Why Use G4F

Advantages of integrating G4F in the AI Chat project:

1. **Cost Reduction**: No expensive API fees
2. **Rapid Prototyping**: Ideal for development and testing phases
3. **Multi-model Support**: Access multiple AI models with one adapter
4. **Flexibility**: Easy to switch between different providers
5. **Active Community**: G4F project is continuously updated and maintained

**âš ï¸ Considerations**:
- G4F is suitable for development and testing environments; official APIs are recommended for production
- Some providers may be unstable or have usage limitations
- Must comply with the terms of service of each AI service provider

### Integration Approaches

#### Approach 1: Direct Integration (Python Subprocess)

Call Python scripts from Node.js to use G4F.

**Pros**:
- Directly uses G4F's Python library
- Full functionality, supports all features
- Easy to update

**Cons**:
- Requires Python environment
- Cross-language communication has performance overhead
- Slightly more complex deployment

#### Approach 2: HTTP Proxy Service

Use G4F's API server mode.

**Pros**:
- Service decoupling, easy to scale
- Can be deployed and scaled independently
- Supports multiple clients sharing

**Cons**:
- Requires additional service management
- Network latency

#### Approach 3: Use Community Node.js Wrapper

Use community-provided Node.js wrapper libraries (like `g4f`).

**Pros**:
- Pure JavaScript/TypeScript implementation
- Simple deployment
- Good performance

**Cons**:
- May not be as feature-complete as the official Python library
- Depends on community maintenance

**This guide recommends Approach 2 (HTTP Proxy Service)** as it provides the best flexibility and maintainability.

### Implementation Steps

See the Chinese version above for detailed implementation steps, including:
1. Installing and starting the G4F API server
2. Creating the G4F adapter
3. Registering the G4F adapter in the session service
4. Configuring environment variables

### Complete Example Code

See the Chinese version above for complete example code including:
- Creating a chat room using G4F
- Testing G4F integration

### Configuration and Usage

#### Docker Compose Configuration

See the Chinese version for the complete `docker-compose.yml` configuration.

#### Supported G4F Models

G4F supports many models, commonly used ones include:

**OpenAI Series**:
- `gpt-4`
- `gpt-4-turbo`
- `gpt-3.5-turbo`

**Anthropic Series**:
- `claude-3-opus`
- `claude-3-sonnet`
- `claude-3-haiku`

**Open Source Models**:
- `llama-2-70b`
- `llama-2-13b`
- `mistral-7b`
- `mixtral-8x7b`

**Google Series**:
- `gemini-pro`
- `palm-2`

### Best Practices

See the Chinese version for detailed best practices including:
1. Error handling and retry logic
2. Request throttling
3. Response caching
4. Monitoring and logging

### FAQ

**Q1: G4F API server won't start**

A: Check the following:
1. Ensure G4F is installed: `pip install -U g4f`
2. Check if port 1337 is in use: `lsof -i :1337`
3. Review G4F logs for detailed error messages

**Q2: Requests timeout or fail**

A:
1. Increase timeout: set `G4F_TIMEOUT` environment variable
2. G4F may automatically switch providers; some may be temporarily unavailable
3. Check network connection and firewall settings

**Q3: Some models are unavailable**

A:
1. G4F provider status changes dynamically
2. Try switching to other models
3. Check the latest G4F project status: https://github.com/xtekky/gpt4free

**Q4: Inconsistent response quality**

A:
1. Quality may vary between providers
2. Consider adding response validation and retry logic
3. Official APIs are recommended for production

**Q5: How to use in production**

A:
G4F is better suited for development and testing. For production:
1. Use official API services
2. Implement official API adapters (OpenAI, Anthropic, etc.)
3. Keep G4F as a fallback option

**Q6: How to handle concurrent requests**

A:
1. Use request queues to limit concurrency
2. Implement rate limiting and backoff strategies
3. Consider deploying multiple G4F instances for load balancing

---

## ç›¸å…³èµ„æº / Related Resources

- [G4F GitHub Repository](https://github.com/xtekky/gpt4free)
- [G4F Documentation](https://github.com/xtekky/gpt4free/blob/main/README.md)
- [Development Documentation](./DEVELOPMENT.md)
- [Main README](./README.md)

## è´¡çŒ® / Contributing

å¦‚æœä½ å‘ç°ä»»ä½•é—®é¢˜æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œæ¬¢è¿æäº¤ Issue æˆ– Pull Requestï¼

If you find any issues or have suggestions for improvement, feel free to submit an Issue or Pull Request!

## è®¸å¯è¯ / License

MIT
